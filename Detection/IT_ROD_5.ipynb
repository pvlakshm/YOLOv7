{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model\n",
        "=====\n",
        "We are using the yolov7 model, and leveraging the implementation from\n",
        "the \"official\" yolov7 repo here: https://github.com/WongKinYiu/yolov7. The repo\n",
        "has the code to do detection, training, generating various metrics with graphs,\n",
        "and the pre-trained weights.\n",
        "\n",
        "Dataset\n",
        "=======\n",
        "We are using the BDD100k dataset. The train, val, test sets have 70K,10K, 20K\n",
        "images respc. The train and val sets come with a json file each, with the\n",
        "annotations.\n",
        "\n",
        "Data preparation\n",
        "================\n",
        "The json file has been converted offline into the yolov7 format.\n",
        "yolov7 format is as follows:\n",
        "1 file per image.\n",
        "Within that file, 1 line for each detected object. Each detected object will\n",
        "be represented by its numberic class_ID, and its bounding box as follows:\n",
        "  class_ID, x_centroid, y_centroid, width, height\n",
        "The centroids, width, and height are normalized using the image dimensions.\n",
        "\n",
        "Training\n",
        "========\n",
        "We use the yolov7.pt weights, and the model has been configured to detect the\n",
        "10 classes called out here: https://doc.bdd100k.com/format.html#object-detection\n",
        "A new file yolov7/data/custom.yaml is used to point to the data used for\n",
        "training.\n",
        "A new file training/custom_yolov7.yaml is used to configure the\n",
        "model to recogmnize the 10 classes\n",
        "\n",
        "Detection/inference\n",
        "===================\n",
        "After training we use the generated best.pt weights for detection.\n",
        "Note the last 5 cells where we do a detection.\n",
        "The logs emitted just below those cells list how many of each class were\n",
        "detected. It also points to the path where the image_with_detection is placed.\n",
        "Those are already opened in this notebook, and you can see the detections.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "3pFWfb6SzKB-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "05ef571c-97ad-4d5a-d57f-32ce779ce217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nModel\\n=====\\nWe are using the yolov7 model, and leveraging the implementation from\\nthe \"official\" yolov7 repo here: https://github.com/WongKinYiu/yolov7. The repo\\nhas the code to do detection, training, generating various metrics with graphs,\\nand the pre-trained weights.\\n\\nDataset\\n=======\\nWe are using the BDD100k dataset. The train, val, test sets have 70K,10K, 20K\\nimages respc. The train and val sets come with a json file each, with the\\nannotations.\\n\\nData preparation\\n================\\nThe json file has been converted offline into the yolov7 format.\\nyolov7 format is as follows:\\n1 file per image.\\nWithin that file, 1 line for each detected object. Each detected object will\\nbe represented by its numberic class_ID, and its bounding box as follows:\\n  class_ID, x_centroid, y_centroid, width, height\\nThe centroids, width, and height are normalized using the image dimensions.\\n\\nTraining\\n========\\nWe use the yolov7.pt weights, and the model has been configured to detect the\\n10 classes called out here: https://doc.bdd100k.com/format.html#object-detection\\nA new file yolov7/data/custom.yaml is used to point to the data used for\\ntraining.\\nA new file training/custom_yolov7.yaml is used to configure the\\nmodel to recogmnize the 10 classes\\n\\nDetection/inference\\n===================\\nAfter training we use the generated best.pt weights for detection.\\nNote the last 5 cells where we do a detection.\\nThe logs emitted just below those cells list how many of each class were\\ndetected. It also points to the path where the image_with_detection is placed.\\nThose are already opened in this notebook, and you can see the detections.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJNGXWX-BUYf",
        "outputId": "2efc1eb5-6d69-4ebd-8f97-5be09f49fdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug  7 04:40:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov7.git"
      ],
      "metadata": {
        "id": "Zq40-J-HKDL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933d01b7-f3a4-4b0e-b48a-fe1066af9230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1191, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1191 (delta 2), reused 6 (delta 2), pack-reused 1185\u001b[K\n",
            "Receiving objects: 100% (1191/1191), 74.23 MiB | 11.49 MiB/s, done.\n",
            "Resolving deltas: 100% (511/511), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/yolov7/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly-RpWRyxz2y",
        "outputId": "73968cb6-b3ca-47eb-9b6b-46ccf347a4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 6)) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 9)) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 10)) (1.10.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 11)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 12)) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 13)) (4.65.0)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 17)) (2.12.3)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 21)) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 22)) (0.12.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 34)) (7.34.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /content/yolov7/requirements.txt (line 35)) (5.9.5)\n",
            "Collecting thop (from -r /content/yolov7/requirements.txt (line 36))\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r /content/yolov7/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r /content/yolov7/requirements.txt (line 9)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r /content/yolov7/requirements.txt (line 9)) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r /content/yolov7/requirements.txt (line 9)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r /content/yolov7/requirements.txt (line 9)) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (16.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (3.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (0.41.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r /content/yolov7/requirements.txt (line 21)) (2022.7.1)\n",
            "Collecting jedi>=0.16 (from ipython->-r /content/yolov7/requirements.txt (line 34))\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r /content/yolov7/requirements.txt (line 34)) (4.8.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r /content/yolov7/requirements.txt (line 34)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r /content/yolov7/requirements.txt (line 34)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r /content/yolov7/requirements.txt (line 34)) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.7.0->-r /content/yolov7/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->-r /content/yolov7/requirements.txt (line 17)) (3.2.2)\n",
            "Installing collected packages: jedi, thop\n",
            "Successfully installed jedi-0.19.0 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30A-4z145tFG",
        "outputId": "97ab7851-77e0-43a5-fd76-a783474466c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights yolov7.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUIgSBJh5XYF",
        "outputId": "0fc2ed3c-9b0d-4326-c9d7-8e0cf0be3ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['yolov7.pt'], source='inference/images/horses.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Downloading https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt to yolov7.pt...\n",
            "100% 72.1M/72.1M [00:05<00:00, 14.0MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "5 horses, Done. (5.7ms) Inference, (24.9ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp/horses.jpg\n",
            "Done. (0.317s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Setup and cleanup code to get the data zip files from Google Drive,\n",
        "and extract them for use from this Google Colab Notebook.\n",
        "\"\"\"\n",
        "def extractzip(zip_path, extract_path):\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  # print('zipfile: ', zip_path)\n",
        "  # print('extract path: ', extract_path)\n",
        "\n",
        "  os.makedirs(extract_path, exist_ok = True)\n",
        "\n",
        "  with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "def setup_data():\n",
        "  # MOUNT_PATH's value is where the drive gets mounted by convention.\n",
        "  # From there onwards, the file paths point to where the zip files are on\n",
        "  # my Google Drive.\n",
        "  # The zip file will be extracted to EXTRACT_PATH. We can then see in\n",
        "  # colab using the Files view on the left.\n",
        "  MOUNT_PATH       = \"/content/drive\"\n",
        "  BDD_ZIPFILE_PATH = MOUNT_PATH + \"/MyDrive/AIML_HYD_20/Capstone_Project/BDD100K/BDD100K_yolov7.zip\"\n",
        "  EXTRACT_PATH     = \"/content\"\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount(MOUNT_PATH)\n",
        "  extractzip(BDD_ZIPFILE_PATH, EXTRACT_PATH)\n",
        "  drive.flush_and_unmount()\n"
      ],
      "metadata": {
        "id": "Fd7-_YlDySUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgGFdj8oykh8",
        "outputId": "4f5120b1-76d4-4094-8b19-9b5d5746684d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = \"/content/images/train\"\n",
        "val_img_path   = \"/content/images/val\"\n",
        "test_img_path  = \"/content/images/test\"\n"
      ],
      "metadata": {
        "id": "mv0bSx2taTkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufr_GchFZ3mB",
        "outputId": "93e2799b-cd9d-4c5b-cb30-6a08e4f5c64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "EkF163vWagJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training images\n",
        "with open(\"/content/train.txt\", \"w\") as f:\n",
        "  img_list = os.listdir(train_img_path)\n",
        "  for img in img_list:\n",
        "    f.write(os.path.join(train_img_path,img+'\\n'))"
      ],
      "metadata": {
        "id": "owGNyKRfaZ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Image\n",
        "with open(\"/content/val.txt\", \"w\") as f:\n",
        "  img_list = os.listdir(val_img_path)\n",
        "  for img in img_list:\n",
        "    f.write(os.path.join(val_img_path,img+'\\n'))"
      ],
      "metadata": {
        "id": "Vx7giAQLaxUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Image\n",
        "with open(\"/content/test.txt\", \"w\") as f:\n",
        "  img_list = os.listdir(test_img_path)\n",
        "  for img in img_list:\n",
        "    f.write(os.path.join(test_img_path,img+'\\n'))"
      ],
      "metadata": {
        "id": "vSQ0TlsmUmTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The below step is not required for the following reason:\n",
        "In an earlier step when we ran a detect on the horses images,\n",
        "the yolov7.pt model was automatically downloaded.\n",
        "If were to not do that step, then we must explicitly download the\n",
        "model as shown below.\n",
        "\"\"\"\n",
        "\n",
        "# # download the yolov7 weights\n",
        "# %cd /content/yolov7\n",
        "# !wget \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\""
      ],
      "metadata": {
        "id": "7wtY7O90bNKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "78b5dd99-5ec3-43e1-e776-19609a485ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe below step is not required for the following reason:\\nIn an earlier step when we ran a detect on the horses images,\\nthe yolov7.pt model was automatically downloaded.\\nIf were to not do that step, then we must explicitly download the\\nmodel as shown below.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/yolov7/data/coco.yaml /content/yolov7/data/custom.yaml\n",
        "\n",
        "# use content as below\n",
        "\"\"\"\n",
        "train: /content/train.txt\n",
        "val: /content/val.txt\n",
        "\n",
        "# number of classes\n",
        "nc: 10\n",
        "\n",
        "# class names\n",
        "names: [ 'pedestrian',\n",
        "         'rider',\n",
        "         'car',\n",
        "         'truck',\n",
        "         'bus',\n",
        "         'train',\n",
        "         'motorcycle',\n",
        "         'bicycle',\n",
        "         'traffic light',\n",
        "         'traffic sign' ]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "HgHEgRQia5QZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "76d92471-ac39-4290-fa83-7d8781f4248f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntrain: /content/train.txt\\nval: /content/val.txt\\n\\n# number of classes\\nnc: 10\\n\\n# class names\\nnames: [ 'pedestrian',\\n         'rider',\\n         'car',\\n         'truck',\\n         'bus',\\n         'train',\\n         'motorcycle',\\n         'bicycle',\\n         'traffic light',\\n         'traffic sign' ]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/yolov7/cfg/training/yolov7.yaml /content/yolov7/cfg/training/custom_yolov7.yaml\n",
        "\n",
        "# Change nc as below\n",
        "\"\"\"\n",
        "nc: 10  # number of classes\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "_vueACdDgR2y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c87ad602-c179-4837-f502-9f808eff9fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnc: 10  # number of classes\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw9ssQgils1E",
        "outputId": "2e175050-e464-4086-b9ae-f15a8307f8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_files_to_drive():\n",
        "\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Source path of the best.pt file\n",
        "    src_weights_file = '/content/yolov7/runs/train/exp3/weights/best.pt'\n",
        "    src_confmatrix_file = '/content/yolov7/runs/train/exp3/confusion_matrix.png'\n",
        "    src_f1curve_file = '/content/yolov7/runs/train/exp3/F1_curve.png'\n",
        "    src_pcurve_file = '/content/yolov7/runs/train/exp3/P_curve.png'\n",
        "    src_prcurve_file = '/content/yolov7/runs/train/exp3/PR_curve.png'\n",
        "    src_rcurve_file = '/content/yolov7/runs/train/exp3/R_curve.png'\n",
        "    src_resultspng_file = '/content/yolov7/runs/train/exp3/results.png'\n",
        "    src_resultstxt_file = '/content/yolov7/runs/train/exp3/results.txt'\n",
        "\n",
        "    # Destination directory on Google Drive\n",
        "    destination_dir = '/content/drive/MyDrive/AIML_HYD_20/Capstone_Project/FinalWeights/'\n",
        "\n",
        "    # Copy the file to the destination directory\n",
        "    shutil.copy(src_weights_file, destination_dir)\n",
        "\n",
        "    # Unmount Google Drive\n",
        "    drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "b3WJjb2zZBIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch 48 --cfg cfg/training/custom_yolov7.yaml --epochs 45 --data /content/yolov7/data/custom.yaml --weights 'yolov7.pt' --device 0\n",
        "copy_files_to_drive()"
      ],
      "metadata": {
        "id": "71e-3ZjZlFtu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e7cc4a7-49d7-4df6-9b1c-7005cd8cf0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-07 05:09:55.340763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-08-07 05:09:55.398683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-07 05:09:56.323790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Namespace(weights='yolov7.pt', cfg='cfg/training/custom_yolov7.yaml', data='/content/yolov7/data/custom.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=45, batch_size=48, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp3', total_batch_size=48)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.75, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 12                -1  1         0  models.common.MP                        []                            \n",
            " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
            " 25                -1  1         0  models.common.MP                        []                            \n",
            " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 38                -1  1         0  models.common.MP                        []                            \n",
            " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
            " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
            " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 76                -1  1         0  models.common.MP                        []                            \n",
            " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
            " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 89                -1  1         0  models.common.MP                        []                            \n",
            " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
            " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
            "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
            "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
            "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
            "105   [102, 103, 104]  1     82702  models.yolo.IDetect                     [10, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 415 layers, 37245102 parameters, 37245102 gradients, 105.3 GFLOPS\n",
            "\n",
            "Transferred 552/566 items from yolov7.pt\n",
            "Scaled weight_decay = 0.000375\n",
            "Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/train.cache' images and labels... 69863 found, 0 missing, 39 empty, 0 corrupted: 100% 69863/69863 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/val.cache' images and labels... 10000 found, 0 missing, 1 empty, 0 corrupted: 100% 10000/10000 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.12, Best Possible Recall (BPR) = 0.9615. Attempting to improve anchors, please wait...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 24164 of 1269018 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 1268740 points...\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9984 best possible recall, 4.45 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.301/0.717-mean/best, past_thr=0.484-mean: 7,8,  17,12,  12,23,  30,22,  26,57,  53,35,  86,61,  135,108,  211,185\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7438: 100% 1000/1000 [02:05<00:00,  7.98it/s]\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9996 best possible recall, 5.19 anchors past thr\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.341/0.744-mean/best, past_thr=0.492-mean: 6,7,  10,8,  9,19,  16,13,  28,21,  19,42,  47,36,  82,58,  156,135\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "\n",
            "Image sizes 640 train, 640 test\n",
            "Using 8 dataloader workers\n",
            "Logging results to runs/train/exp3\n",
            "Starting training for 45 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      0/44     5.11G   0.04856   0.02877  0.008357   0.08569       568       640: 100% 1456/1456 [23:58<00:00,  1.01it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:27<00:00,  1.20it/s]\n",
            "                 all       10000      185671        0.65       0.427       0.445       0.226\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      1/44     37.7G   0.03718   0.02822  0.003805   0.06921       737       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.33it/s]\n",
            "                 all       10000      185671       0.681       0.452       0.482       0.253\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      2/44     40.1G    0.0368   0.02785  0.003318   0.06797       732       640: 100% 1456/1456 [23:44<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.673       0.458       0.486       0.253\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      3/44     40.1G   0.03687   0.02758  0.003098   0.06755       828       640: 100% 1456/1456 [23:40<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.701       0.458       0.498       0.263\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      4/44     40.1G   0.03637   0.02784  0.002897   0.06711       793       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.33it/s]\n",
            "                 all       10000      185671       0.722        0.48       0.524        0.28\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      5/44     40.1G    0.0361   0.02773  0.002771    0.0666       812       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.732       0.486       0.534       0.286\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      6/44     40.1G    0.0358   0.02803    0.0027   0.06653       882       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.732       0.496       0.545       0.297\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      7/44     40.1G   0.03548   0.02812  0.002619   0.06622       986       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.745       0.501       0.553       0.302\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      8/44     40.1G   0.03531   0.02825   0.00256   0.06613       721       640: 100% 1456/1456 [23:36<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.748       0.504       0.559       0.307\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "      9/44     40.1G   0.03511   0.02833  0.002472   0.06591       725       640: 100% 1456/1456 [23:34<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.732       0.519       0.562        0.31\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     10/44     40.1G   0.03489   0.02835   0.00242   0.06567       774       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.752       0.512       0.566       0.313\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     11/44     40.1G   0.03475   0.02842  0.002394   0.06556       790       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671        0.73       0.529       0.569       0.316\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     12/44     40.1G   0.03454   0.02833  0.002364   0.06523       698       640: 100% 1456/1456 [23:37<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.744       0.521        0.57       0.317\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     13/44     40.1G   0.03449   0.02835  0.002321   0.06516       596       640: 100% 1456/1456 [23:34<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.732       0.531       0.571       0.318\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     14/44     40.1G   0.03443   0.02836  0.002305    0.0651       934       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.723        0.54       0.572       0.319\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     15/44     40.1G   0.03422   0.02835  0.002269   0.06484       965       640: 100% 1456/1456 [23:33<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.739        0.53       0.573       0.319\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     16/44     40.1G   0.03419   0.02814  0.002259   0.06458       676       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.739       0.533       0.574        0.32\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     17/44     40.1G   0.03404    0.0282  0.002223   0.06446       792       640: 100% 1456/1456 [23:36<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671        0.73        0.54       0.575       0.321\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     18/44     40.1G   0.03404   0.02817   0.00223   0.06444       808       640: 100% 1456/1456 [23:37<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.735        0.54       0.576       0.321\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     19/44     40.1G   0.03401    0.0281  0.002202   0.06431       965       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.748       0.531       0.578       0.322\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     20/44     40.1G   0.03394   0.02804  0.002183   0.06416       673       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.738       0.539       0.578       0.323\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     21/44     40.1G   0.03382    0.0282   0.00217   0.06419       798       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671        0.74       0.537       0.579       0.324\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     22/44     40.1G    0.0338   0.02797   0.00214   0.06391       947       640: 100% 1456/1456 [23:36<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.739       0.541       0.581       0.324\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     23/44     40.1G   0.03376   0.02804  0.002112   0.06392       766       640: 100% 1456/1456 [23:35<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.33it/s]\n",
            "                 all       10000      185671       0.754       0.533       0.582       0.325\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     24/44     40.1G   0.03373   0.02799  0.002102   0.06382       864       640: 100% 1456/1456 [23:34<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.757       0.531       0.582       0.325\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     25/44     40.1G   0.03359   0.02785  0.002058    0.0635       829       640: 100% 1456/1456 [23:37<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671        0.76        0.53       0.583       0.326\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     26/44     40.1G   0.03362    0.0279  0.002051   0.06357      1035       640: 100% 1456/1456 [23:32<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.763       0.529       0.583       0.326\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     27/44     40.1G    0.0335   0.02774  0.002031   0.06327       935       640: 100% 1456/1456 [23:36<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.734       0.548       0.584       0.327\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     28/44     40.1G   0.03336   0.02772  0.002018    0.0631       936       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671        0.74       0.544       0.584       0.327\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     29/44     40.1G   0.03332   0.02763   0.00199   0.06294       845       640: 100% 1456/1456 [23:40<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.737       0.547       0.584       0.328\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     30/44     40.1G   0.03331   0.02754  0.001983   0.06283       803       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.738       0.548       0.585       0.328\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     31/44     40.1G   0.03326   0.02764  0.001965   0.06286       725       640: 100% 1456/1456 [23:40<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.745       0.544       0.586       0.329\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     32/44     40.1G   0.03319    0.0275  0.001944   0.06263       826       640: 100% 1456/1456 [23:39<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.739        0.55       0.586       0.329\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     33/44     40.1G   0.03305   0.02757  0.001928   0.06255      1062       640: 100% 1456/1456 [23:42<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.742       0.549       0.587        0.33\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     34/44     40.1G     0.033   0.02752  0.001904   0.06242       684       640: 100% 1456/1456 [23:42<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.741       0.551       0.589       0.331\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     35/44     40.1G   0.03293   0.02756  0.001898   0.06238       665       640: 100% 1456/1456 [23:42<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671        0.75       0.546        0.59       0.331\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     36/44     40.1G   0.03294   0.02729  0.001871    0.0621       870       640: 100% 1456/1456 [23:41<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.762       0.538       0.593       0.334\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     37/44     40.1G   0.03288   0.02747  0.001861    0.0622       859       640: 100% 1456/1456 [23:39<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.767       0.537       0.597       0.336\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     38/44     40.1G   0.03285   0.02734  0.001848   0.06205       914       640: 100% 1456/1456 [23:41<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.35it/s]\n",
            "                 all       10000      185671       0.758       0.544       0.597       0.336\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     39/44     40.1G   0.03273   0.02738  0.001809   0.06191       716       640: 100% 1456/1456 [23:44<00:00,  1.02it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.756       0.545       0.599       0.338\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     40/44     40.1G   0.03265   0.02735  0.001802    0.0618       921       640: 100% 1456/1456 [23:39<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.764       0.539       0.599       0.339\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     41/44     40.1G   0.03256   0.02728   0.00179   0.06163       691       640: 100% 1456/1456 [23:38<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.737       0.558         0.6        0.34\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     42/44     40.1G   0.03264   0.02719  0.001783   0.06161       699       640: 100% 1456/1456 [23:39<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:18<00:00,  1.34it/s]\n",
            "                 all       10000      185671       0.745       0.553       0.601        0.34\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     43/44     40.1G   0.03256   0.02708  0.001773   0.06141       812       640: 100% 1456/1456 [23:37<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [01:17<00:00,  1.36it/s]\n",
            "                 all       10000      185671       0.743       0.555       0.602       0.341\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     44/44     40.1G   0.03251   0.02722  0.001777    0.0615       759       640: 100% 1456/1456 [23:39<00:00,  1.03it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 105/105 [02:00<00:00,  1.15s/it]\n",
            "                 all       10000      185671       0.751        0.55       0.604       0.341\n",
            "          pedestrian       10000       13398        0.77       0.615       0.701       0.349\n",
            "               rider       10000         655       0.621        0.51       0.506        0.25\n",
            "                 car       10000      102684       0.846       0.762        0.84       0.526\n",
            "               truck       10000        4231       0.717       0.641       0.699        0.51\n",
            "                 bus       10000        1656       0.747       0.603       0.683       0.522\n",
            "               train       10000          15           1           0       0.102      0.0706\n",
            "          motorcycle       10000         460       0.729       0.487       0.539       0.259\n",
            "             bicycle       10000        1035       0.618       0.506       0.525       0.263\n",
            "       traffic light       10000       26857       0.719        0.67       0.689       0.262\n",
            "        traffic sign       10000       34680        0.74       0.704       0.754       0.403\n",
            "45 epochs completed in 18.768 hours.\n",
            "\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 74.9MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 74.9MB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e4cdd3ac2e67>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python train.py --batch 48 --cfg cfg/training/custom_yolov7.yaml --epochs 45 --data /content/yolov7/data/custom.yaml --weights 'yolov7.pt' --device 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcopy_files_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-dfcf99330ca2>\u001b[0m in \u001b[0;36mcopy_files_to_drive\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Source path of the best.pt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ6EylhhMcFL",
        "outputId": "67742797-dd38-4335-ad83-0b97305c3ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python detect.py --weights /content/yolov7/runs/train/exp/weights/best.pt --source /content/images/test/cabc30fc-e7726578.jpg\n",
        "# !python detect.py --weights /content/yolov7/runs/train/exp/weights/best.pt --source /content/images/test/cabc30fc-eb673c5a.jpg\n",
        "# !python detect.py --weights /content/yolov7/runs/train/exp/weights/best.pt --source /content/images/test/cabc30fc-fd79926f.jpg\n",
        "# !python detect.py --weights /content/yolov7/runs/train/exp/weights/best.pt --source /content/images/test/cabc9045-1b8282ba.jpg\n",
        "# !python detect.py --weights /content/yolov7/runs/train/exp/weights/best.pt --source /content/images/test/cabc9045-581f64de.jpg\n"
      ],
      "metadata": {
        "id": "tom1rsknNGor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "#drive.mount('/content/drive')\n",
        "#drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "AmutD0eeBhoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source path of the best.pt file\n",
        "src_weights_file = '/content/yolov7/runs/train/exp3/weights/best.pt'\n",
        "src_confmatrix_file = '/content/yolov7/runs/train/exp3/confusion_matrix.png'\n",
        "src_f1curve_file = '/content/yolov7/runs/train/exp3/F1_curve.png'\n",
        "src_pcurve_file = '/content/yolov7/runs/train/exp3/P_curve.png'\n",
        "src_prcurve_file = '/content/yolov7/runs/train/exp3/PR_curve.png'\n",
        "src_rcurve_file = '/content/yolov7/runs/train/exp3/R_curve.png'\n",
        "src_resultspng_file = '/content/yolov7/runs/train/exp3/results.png'\n",
        "src_resultstxt_file = '/content/yolov7/runs/train/exp3/results.txt'\n",
        "\n",
        "# Destination directory on Google Drive\n",
        "destination_dir = '/content/drive/MyDrive/AIML_HYD_20/Capstone_Project/FinalWeights/'\n",
        "\n",
        "# Copy the file to the destination directory\n",
        "shutil.copy(src_weights_file, destination_dir)\n",
        "\n",
        "# Unmount Google Drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "71hNOFenBgNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0985301c-3f08-4a65-b79c-da73070fb646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "copy model\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cp /content/drive/MyDrive/AIML_HYD_20/Capstone_Project/FinalWeights/best.pt /content/yolov7\n",
        "%cp /content/drive/MyDrive/AIML_HYD_20/Capstone_Project/traffic.mp4 /content/.\n",
        "drive.flush_and_unmount()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWKKS-nUQYA7",
        "outputId": "daebda4e-ac6b-40d6-b6de-dfc002d7056c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights best.pt --source /content/images/test/cabc30fc-e7726578.jpg\n",
        "!python detect.py --weights best.pt --source /content/images/test/cabc30fc-eb673c5a.jpg\n",
        "!python detect.py --weights best.pt --source /content/images/test/cabc30fc-fd79926f.jpg\n",
        "!python detect.py --weights best.pt --source /content/images/test/cabc9045-1b8282ba.jpg\n",
        "!python detect.py --weights best.pt --source /content/images/test/cabc9045-581f64de.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbvGcp2huSGC",
        "outputId": "69043f09-c72a-4a74-cd39-20934976f47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['best.pt'], source='/content/images/test/cabc30fc-e7726578.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "2 pedestrians, 6 cars, 2 traffic signs, Done. (5.8ms) Inference, (1.5ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp7/cabc30fc-e7726578.jpg\n",
            "Done. (0.236s)\n",
            "Namespace(weights=['best.pt'], source='/content/images/test/cabc30fc-eb673c5a.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "10 cars, 2 trucks, 4 traffic signs, Done. (5.8ms) Inference, (1.6ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp8/cabc30fc-eb673c5a.jpg\n",
            "Done. (0.239s)\n",
            "Namespace(weights=['best.pt'], source='/content/images/test/cabc30fc-fd79926f.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "4 pedestrians, 1 rider, 6 cars, 1 truck, 1 motorcycle, 1 bicycle, Done. (5.8ms) Inference, (1.5ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp9/cabc30fc-fd79926f.jpg\n",
            "Done. (0.239s)\n",
            "Namespace(weights=['best.pt'], source='/content/images/test/cabc9045-1b8282ba.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "22 cars, 6 traffic lights, 3 traffic signs, Done. (5.7ms) Inference, (1.5ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp10/cabc9045-1b8282ba.jpg\n",
            "Done. (0.237s)\n",
            "Namespace(weights=['best.pt'], source='/content/images/test/cabc9045-581f64de.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "6 cars, 2 traffic lights, 4 traffic signs, Done. (5.9ms) Inference, (1.5ms) NMS\n",
            " The image with the result is saved in: runs/detect/exp11/cabc9045-581f64de.jpg\n",
            "Done. (0.234s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights best.pt --source /content/traffic.mp4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8O-ItXQtskL",
        "outputId": "9049ce3d-c3dd-4c47-a88c-da6ee543e7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(weights=['best.pt'], source='/content/traffic.mp4', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
            "YOLOR ðŸš€ v0.1-126-g84932d7 torch 2.0.1+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40513.5625MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 314 layers, 36530318 parameters, 6194944 gradients, 103.3 GFLOPS\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "video 1/1 (1/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.6ms) Inference, (1.5ms) NMS\n",
            "video 1/1 (2/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (6.2ms) Inference, (1.1ms) NMS\n",
            "video 1/1 (3/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.1ms) NMS\n",
            "video 1/1 (4/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.1ms) NMS\n",
            "video 1/1 (5/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.1ms) NMS\n",
            "video 1/1 (6/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (7/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (8/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (9/125) /content/traffic.mp4: 5 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (10/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (11/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (12/125) /content/traffic.mp4: 5 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (13/125) /content/traffic.mp4: 5 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (14/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (15/125) /content/traffic.mp4: 5 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (16/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (17/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (18/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (19/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (20/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (21/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (22/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (23/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (24/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (25/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (26/125) /content/traffic.mp4: 8 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (27/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (28/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (29/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (30/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (31/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (32/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (33/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (34/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (35/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (36/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (37/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (38/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (6.0ms) Inference, (1.1ms) NMS\n",
            "video 1/1 (39/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (40/125) /content/traffic.mp4: 7 cars, 1 truck, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (41/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (42/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (43/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (44/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (45/125) /content/traffic.mp4: 8 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (46/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (47/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.2ms) NMS\n",
            "video 1/1 (48/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (49/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (50/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (51/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (52/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (53/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (54/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (55/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (56/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (57/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (58/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (59/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (60/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (61/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (62/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (6.0ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (63/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (64/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (65/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (66/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (67/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (68/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (69/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (70/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (71/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (72/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (73/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (74/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (75/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (76/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (77/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (78/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (79/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (80/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (81/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (82/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (83/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (84/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (85/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (86/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (87/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (88/125) /content/traffic.mp4: 5 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (89/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (90/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (91/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (92/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (6.3ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (93/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (94/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (95/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (96/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (97/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (98/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (99/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (100/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.3ms) NMS\n",
            "video 1/1 (101/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.9ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (102/125) /content/traffic.mp4: 5 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (103/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (104/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (105/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (106/125) /content/traffic.mp4: 6 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (107/125) /content/traffic.mp4: 7 cars, 5 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (108/125) /content/traffic.mp4: 8 cars, 1 truck, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (109/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (110/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (111/125) /content/traffic.mp4: 8 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (112/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (113/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (114/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (115/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (116/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (117/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (118/125) /content/traffic.mp4: 6 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (119/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (120/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (121/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (122/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.8ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (123/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (124/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "video 1/1 (125/125) /content/traffic.mp4: 7 cars, 6 traffic signs, Done. (5.7ms) Inference, (1.0ms) NMS\n",
            "Done. (2.148s)\n"
          ]
        }
      ]
    }
  ]
}